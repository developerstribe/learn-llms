# Ollama runs LLMs locally

Besides using ChatGPT (which can cost money), you can use LLMs models locally with various tools, such as Ollama. While it may not match the performance and output of using LLMs via OpenAI or others (that run models with higher parameters and more precision), it's good enough for local applications.

With Ollama, we can:

- use deep thinking models
- not worry about token usage i.e. use freely, not get tracked, etc

## Learn

- How Ollama models are built?
- How can they run in commodity laptops (and does not need massive GPUs)?

## Next

In the [next course](../01-prompt-engineering/README.md), we will use Ollama to try out couple prompt engineering techniques.